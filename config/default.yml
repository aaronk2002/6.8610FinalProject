batch_size: 8
debug: False
dropout: 0.1
embedding_dim: 1000
epochs: 100
event_dim: 388
experiment: embedding256-layer6
fp16: None
l_r: 0.001
label_smooth: 0.1
load_path: None
max_seq: 2048
num_layers: 6
pad_token: 388
pickle_dir: dataset/processed
token_eos: 390
token_sos: 389
vocab_size: 391